# -*- coding: utf-8 -*-
"""FinalCodeCapcoHackathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gY84PG5nkeNJQ3v0Ucypmc0RpXL8LpYb

**LOAN PREDICTION**

**Importing Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")



from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, StratifiedKFold,train_test_split
from sklearn.metrics import roc_auc_score, accuracy_score ,confusion_matrix, roc_curve,precision_score,recall_score,precision_recall_curve
import warnings
warnings.filterwarnings("ignore")

"""*Ingesting Source data and Loading as a dataframe*"""

df=pd.read_csv("LPD_Merged.csv")

"""Replacing values in Coulumns (Years in current job,Home Ownership) to ensure Standard Names"""

dict_yrsinjob = {'10+ years':11,'8 years':9,'9 years':10,'7 years':8,'6 years':7,'5 years':6,'4 years':5,'3 years':4,'2 years':3,'1 year':2,'< 1 year':1}
df.replace({'Years in current job':dict_yrsinjob}, inplace =True)

dict_hometype = {'HaveMortgage':'Home Mortgage'}
df.replace({'Home Ownership':dict_hometype}, inplace =True)

"""Checking for Missing Values"""

select_coloumns = ['Current Loan Amount','Term','Credit Score','household_income','Years in current job','Home Ownership','Loan_type','Monthly Debt','Years of Credit History','Loan Status','gender','age']

df=df[select_coloumns]
df



"""Renaming Columns"""

df = df.rename(columns={'Current Loan Amount': 'CurrentLoanAmount', 'Credit Score': 'CreditScore',
                        'Years in current job': 'YearsInCurrentJob',
                        'Home Ownership': 'HomeOwnership',
                       'Monthly Debt': 'Monthly_Debt',
                        'Years of Credit History': 'YearsOfCreditHistory',
                        'Loan Status': 'LoanStatus','gender': 'Gender','age':'Age'})

"""Checking for null values"""

pd.isnull(df).sum()

"""Dropping Null values (About 27000 rows were dropped).Safe to drop as we still have 43K rows left which enoufg to train the model"""

df_drop=df.dropna(axis=0)
df.shape,df_drop.shape

df=df_drop

"""# New Section

**Exploratory Data Analysis (EDA)**

> Checking Column header names
"""

df.columns

"""

> Checking Data Types of Columns



"""

df.dtypes

df['Age'] = df['Age'].apply(pd.to_numeric)

df.dtypes

"""

> Check the Number of Columns and Row
"""

df.shape

"""

> Check the Split of the target variable

"""

df['LoanStatus'].value_counts()

df['LoanStatus'].value_counts().plot.bar()

Term=pd.crosstab(df['Term'],df['LoanStatus'])
Term.div(Term.sum(1).astype(float),axis=0).plot(kind="bar",stacked=True,figsize=(4,4))

Term=pd.crosstab(df['Gender'],df['LoanStatus'])
Term.div(Term.sum(1).astype(float),axis=0).plot(kind="bar",stacked=True,figsize=(4,4))

"""We see a higher rate of defaulters for Long term loans which may indicate that the term might be a important parameter in determining default rate"""

Term=pd.crosstab(df['YearsInCurrentJob'],df['LoanStatus'])
Term.div(Term.sum(1).astype(float),axis=0).plot(kind="bar",stacked=True,figsize=(4,4))

"""We dont see much varaiation in default rate when we consider the YearsInCurrentJob.This might not be a important parmeter or might not significantly determine whether a loan is defaulted or not"""

Term=pd.crosstab(df['HomeOwnership'],df['LoanStatus'])
Term.div(Term.sum(1).astype(float),axis=0).plot(kind="bar",stacked=True,figsize=(4,4))

"""We do see some variation in default rate when we consider Home ownership with people living underÌ¥ rent tending to default more compared to ones living in homes under morgtgage.

"""

Term=pd.crosstab(df['Loan_type'],df['LoanStatus'])
Term.div(Term.sum(1).astype(float),axis=0).plot(kind="bar",stacked=True,figsize=(4,4))

"""We do see some variation in default rate when we consider Loan type with people taking business loans tending to default more while Home loans (typical low risk instrument) and Education loan having lower default rate"""

plt.subplot(121)
sns.distplot(df['CreditScore']);
plt.subplot(122)
df['CreditScore'].plot.box(figsize=(16,5));
plt.show()

plt.subplot(121)
sns.distplot(df['CurrentLoanAmount']);
plt.subplot(122)
df['CurrentLoanAmount'].plot.box(figsize=(16,5));
plt.show()

plt.subplot(121)
sns.distplot(df['household_income']);
plt.subplot(122)
df['household_income'].plot.box(figsize=(16,5));
plt.show()

plt.subplot(121)
sns.distplot(df['Monthly_Debt']);
plt.subplot(122)
df['Monthly_Debt'].plot.box(figsize=(16,5));
plt.show()

plt.subplot(121)
sns.distplot(df['YearsOfCreditHistory']);
plt.subplot(122)
df['YearsOfCreditHistory'].plot.box(figsize=(16,5));
plt.show()

df.groupby('LoanStatus')['household_income'].mean().plot.bar()

df.groupby('LoanStatus')['CreditScore'].mean().plot.bar()

df.groupby('LoanStatus')['Age'].mean().plot.bar()

df.groupby('LoanStatus')['YearsInCurrentJob'].mean().plot.bar()

df.groupby('LoanStatus')['Monthly_Debt'].mean().plot.bar()

df.groupby('LoanStatus')['YearsOfCreditHistory'].mean().plot.bar()

df.dtypes

pd.isnull(df).sum()

df['LoanStatus'].replace('Fully Paid',1,inplace=True)
df['LoanStatus'].replace('Charged Off',0,inplace=True)

df1=pd.get_dummies(df_drop)

df=df1

df

pd.isnull(df).sum()

df=df.reset_index(drop=True)
df

df.describe().T

sns.boxplot(x=df['household_income'])

sns.distplot(df['household_income'],bins=10,kde= False)
plt.show()

"""# New Section

Feature Engineering-Adding a New column which calculates the net income
"""

df['NetIncome'] = df['household_income'] - df['Monthly_Debt']
df.describe()

df

"""Handling Outliers by trimming"""

# setting a different outlier range for the IQR method
Q1 = df['Age'].quantile(0.25)
Q3 = df['Age'].quantile(0.75)
IQR = Q3 - Q1

# creating a new dataframe to hold the trimmed data
df_trimmed = pd.DataFrame(columns=df.columns)

# appending rows that are within the IQR range to the new dataframe
for i in range(0, df.shape[0]):
    if df.loc[i, 'Age'] <= Q3 + 1.5 * IQR:
        df_trimmed = df_trimmed.append(df.loc[i])

print("Original DataFrame:")
print(df)
print("\nTrimmed DataFrame:")
print(df_trimmed)

df=df_trimmed

"""Normalization by Scaling (Min Max)"""

from sklearn.preprocessing import MinMaxScaler
num_cols = df.select_dtypes(include=['int64', 'float64']).columns

scaler = MinMaxScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

"""Taking a dump to csv.This file is used for training in Azure ML studio and further deployment was done in Azure ML studio"""

df.to_csv('NewDataAzureML01112023_01.csv')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

"""Training Model and Prediction"""

# Split the data into train and test sets
X = df.drop('LoanStatus', axis=1)
y = df['LoanStatus']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Create and train Logistic Regression model
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

# Create and train Random Forest model
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Predict the test set results for both models
lr_pred = lr_model.predict(X_test)
rf_pred = rf_model.predict(X_test)

"""Printing Metrics"""

#Calculate the accuracy, precision, recall, F1 score, ROC AUC curve details and confusion matrix for both models
print('Accuracy')
print('Logistic Regression: ', accuracy_score(y_test, lr_pred))
print('Random Forest: ', accuracy_score(y_test, rf_pred))

print('Precision')
print('Logistic Regression: ', precision_score(y_test, lr_pred))
print('Random Forest: ', precision_score(y_test, rf_pred))

print('Recall')
print('Logistic Regression: ', recall_score(y_test, lr_pred))
print('Random Forest: ', recall_score(y_test, rf_pred))

print('F1 Score')
print('Logistic Regression: ', f1_score(y_test, lr_pred))
print('Random Forest: ', f1_score(y_test, rf_pred))

print('Confusion Matrix')
print('Logistic Regression: ', confusion_matrix(y_test, lr_pred))
print('Random Forest: ', confusion_matrix(y_test, rf_pred))

"""HyperParameter Tuning"""

# Split the data into train and test sets
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
X_train1, X_test1, y_train1, y_test1 = train_test_split(df.drop('LoanStatus', axis=1), df['LoanStatus'], test_size=0.2, random_state=42)
# Create a dictionary of models
models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier()
}

# Create a dictionary of hyperparameter grids
param_grids = {
    'Logistic Regression': {
        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]
    },
    'Random Forest': {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 7],
        'min_samples_split': [2, 4, 6],
        'min_samples_leaf': [1, 2, 4]
    }
}

"""Improting Pickle File"""

import pickle
pickle_out = open("rf_model.pkl","wb")
pickle.dump(rf_model, pickle_out)
pickle_out.close()